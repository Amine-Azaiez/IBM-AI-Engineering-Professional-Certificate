{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42af050f3b9845ef922ee99f46c5322f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2956d3f74e41ba95de7d846255ec7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',  # Path to the validation directory\n",
    "    target_size=(image_resize, image_resize),  # Resize images to 224 x 224\n",
    "    batch_size=batch_size_validation,          # Batch size for validation\n",
    "    class_mode='categorical'                   # Categorical classification\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 18:57:30.376979: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2025-01-19 18:57:30.386480: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394300000 Hz\n",
      "2025-01-19 18:57:30.387027: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557a50a6ce60 executing computations on platform Host. Devices:\n",
      "2025-01-19 18:57:30.387066: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2025-01-19 18:57:30.427927: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f5900e27850>,\n",
       " <keras.layers.core.Dense at 0x7f58fa1068d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f5979b512d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f5973ce9850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5971c7f910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5971c7f750>,\n",
       " <keras.layers.core.Activation at 0x7f5973c96890>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f5973ce97d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59732a3990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59731b2dd0>,\n",
       " <keras.layers.core.Activation at 0x7f59731b2a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59731caa50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5973131e10>,\n",
       " <keras.layers.core.Activation at 0x7f5973131ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5970431f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59702ca650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59703abd50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59702eb990>,\n",
       " <keras.layers.merge.Add at 0x7f597021f950>,\n",
       " <keras.layers.core.Activation at 0x7f59701c87d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59701c8510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5970147110>,\n",
       " <keras.layers.core.Activation at 0x7f5970147250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59700e2910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f597005a850>,\n",
       " <keras.layers.core.Activation at 0x7f5970026690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59607efc50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5960753690>,\n",
       " <keras.layers.merge.Add at 0x7f5960753790>,\n",
       " <keras.layers.core.Activation at 0x7f59606eadd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59606ea990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f596066bb90>,\n",
       " <keras.layers.core.Activation at 0x7f596066b110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5960583790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5960567f90>,\n",
       " <keras.layers.core.Activation at 0x7f59604d8bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f596049bed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59603ff650>,\n",
       " <keras.layers.merge.Add at 0x7f59603ff750>,\n",
       " <keras.layers.core.Activation at 0x7f5960394910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5960394a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5960316690>,\n",
       " <keras.layers.core.Activation at 0x7f59603160d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59602b12d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5960211f50>,\n",
       " <keras.layers.core.Activation at 0x7f59733c8950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f596019fb90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5960099a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59601025d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5960052150>,\n",
       " <keras.layers.merge.Add at 0x7f5960052bd0>,\n",
       " <keras.layers.core.Activation at 0x7f593c75f910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f593c683190>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f593c6f2090>,\n",
       " <keras.layers.core.Activation at 0x7f593c6db650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f593c672dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f593c539b50>,\n",
       " <keras.layers.core.Activation at 0x7f593c5ee8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f593c50a250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f593c4eec10>,\n",
       " <keras.layers.merge.Add at 0x7f593c4ee550>,\n",
       " <keras.layers.core.Activation at 0x7f593c426bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f593c75fe50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f593c39d0d0>,\n",
       " <keras.layers.core.Activation at 0x7f593c3850d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f593c3208d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f593c28cb10>,\n",
       " <keras.layers.core.Activation at 0x7f593c28cf10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f593c235290>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f593c1547d0>,\n",
       " <keras.layers.merge.Add at 0x7f593c19b9d0>,\n",
       " <keras.layers.core.Activation at 0x7f593c0d1dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f593c059650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f593c04a250>,\n",
       " <keras.layers.core.Activation at 0x7f593c0b2090>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f592478be50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5924777a90>,\n",
       " <keras.layers.core.Activation at 0x7f5924777e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59246a0890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59245c2710>,\n",
       " <keras.layers.merge.Add at 0x7f5924607910>,\n",
       " <keras.layers.core.Activation at 0x7f59245a3c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59245a3b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f592451c3d0>,\n",
       " <keras.layers.core.Activation at 0x7f592451c790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f592443bdd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5924420a10>,\n",
       " <keras.layers.core.Activation at 0x7f5924420e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5924350790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f592426a890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59242f1c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59241ef6d0>,\n",
       " <keras.layers.merge.Add at 0x7f5924350510>,\n",
       " <keras.layers.core.Activation at 0x7f59241657d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5924165510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f592407e0d0>,\n",
       " <keras.layers.core.Activation at 0x7f5924041f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59027e9a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f590273d450>,\n",
       " <keras.layers.core.Activation at 0x7f590273d490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59026dbc50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5902654ed0>,\n",
       " <keras.layers.merge.Add at 0x7f5902674050>,\n",
       " <keras.layers.core.Activation at 0x7f59025f2f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59024fd5d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5902558d10>,\n",
       " <keras.layers.core.Activation at 0x7f5902558590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59024892d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59024257d0>,\n",
       " <keras.layers.core.Activation at 0x7f590246d9d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5902389c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5902302310>,\n",
       " <keras.layers.merge.Add at 0x7f5902302450>,\n",
       " <keras.layers.core.Activation at 0x7f5902299950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5902299c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5902219f10>,\n",
       " <keras.layers.core.Activation at 0x7f5902219bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59021b4e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59020fbad0>,\n",
       " <keras.layers.core.Activation at 0x7f590212c190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f590204a590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f590202cbd0>,\n",
       " <keras.layers.merge.Add at 0x7f590202c510>,\n",
       " <keras.layers.core.Activation at 0x7f5901f4aa10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59021b4f50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5901ec3c90>,\n",
       " <keras.layers.core.Activation at 0x7f5901ec3350>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5901e7ad10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5901d85210>,\n",
       " <keras.layers.core.Activation at 0x7f5901ddd7d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5901d754d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5901c3eb10>,\n",
       " <keras.layers.merge.Add at 0x7f5901cf0690>,\n",
       " <keras.layers.core.Activation at 0x7f5901c0d5d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5901b53890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5901ba8d10>,\n",
       " <keras.layers.core.Activation at 0x7f5901ba8b90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5901b23a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5901a8ae50>,\n",
       " <keras.layers.core.Activation at 0x7f5901a8ad10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5901a39ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f590194a690>,\n",
       " <keras.layers.merge.Add at 0x7f59019a2e90>,\n",
       " <keras.layers.core.Activation at 0x7f5901935a50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59018dbf90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59018b24d0>,\n",
       " <keras.layers.core.Activation at 0x7f59018b2910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59017f8a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f590174b450>,\n",
       " <keras.layers.core.Activation at 0x7f590174bfd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59016e6c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f590157f410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f590164efd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f590159a750>,\n",
       " <keras.layers.merge.Add at 0x7f59015b3ed0>,\n",
       " <keras.layers.core.Activation at 0x7f5901480a50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59014b5650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59013fdf50>,\n",
       " <keras.layers.core.Activation at 0x7f59013fd490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59013b1290>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f590130ff10>,\n",
       " <keras.layers.core.Activation at 0x7f590130f950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f59012aedd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5901228210>,\n",
       " <keras.layers.merge.Add at 0x7f59012282d0>,\n",
       " <keras.layers.core.Activation at 0x7f5901142590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5901142ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f59010bea90>,\n",
       " <keras.layers.core.Activation at 0x7f59010bed90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f590105d790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5900fc5d50>,\n",
       " <keras.layers.core.Activation at 0x7f5900fd7b10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f5900f717d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f5900ed5ad0>,\n",
       " <keras.layers.merge.Add at 0x7f5900ed5e90>,\n",
       " <keras.layers.core.Activation at 0x7f5900e72c90>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f5901142050>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f593c426a90>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 18:57:48.594591: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/101 [..............................] - ETA: 1:48:04 - loss: 0.9670 - acc: 0.4100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 18:58:48.390873: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/101 [..............................] - ETA: 1:41:28 - loss: 0.8851 - acc: 0.4500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 18:59:46.558041: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/101 [..............................] - ETA: 1:38:24 - loss: 0.8432 - acc: 0.4767"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 19:00:44.349880: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/101 [>.............................] - ETA: 1:36:19 - loss: 0.7692 - acc: 0.5400"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 19:01:41.893128: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 9501s 94s/step - loss: 0.0981 - acc: 0.9636 - val_loss: 0.5052 - val_acc: 0.7754\n",
      "Epoch 2/2\n",
      "100/101 [============================>.] - ETA: 59s - loss: 0.0287 - acc: 0.9933 "
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
